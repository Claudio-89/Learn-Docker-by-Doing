##########################
Setting Up a Docker Swarm
##########################
For the last six months, the Acme Anvil Corporation has been migrating some of their bare metal infrastructure to Docker containers. A schism has developed between the members of your team on whether to use Docker Swarm or Kubernetes. Your manager has decided to settle the dispute by creating two competing demos for Docker Swarm and Kubernetes. You have been tasked with helping to create the Docker Swarm demo. Create a swarm with three nodes and then create a service using the weather-app image.

"Docker Swarm Overview: A Docker Swarm is a group of either physical or virtual machines that are running the Docker application and that have been configured to join together in a cluster. ... Docker swarm is a container orchestration tool, meaning that it allows the user to manage multiple containers deployed across multiple host machines"


## Initialize the Docker swarm - Swarm Server 1 ##

[root@ip-10-0-1-114 ~]# docker swarm init
Swarm initialized: current node (z237n429u62p9eylkakpsmzg6) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-02ong7axnkouffgbxw43u6zowoa32w3iwdat7w7y44fud0k9bg-d8v6f2i0f6xsw7wj0lxgoe3gj 10.0.1.114:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

[root@ip-10-0-1-114 ~]# docker node ls
ID                            HOSTNAME                     STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
z237n429u62p9eylkakpsmzg6 *   ip-10-0-1-114.ec2.internal   Ready               Active              Leader              19.03.8
mybreple3ama75xr2y4pu0i9u     ip-10-0-1-142.ec2.internal   Ready               Active                                  19.03.8
s6dhut65uhzt4533r6havdwx3     ip-10-0-1-164.ec2.internal   Ready               Active                                  19.03.8

## Create a swarm service ##

[root@ip-10-0-1-114 ~]# docker service create --name weather-app --publish published=80,target=3000 --replicas=3 weather-app
image weather-app:latest could not be accessed on a registry to record
its digest. Each node will access weather-app:latest independently,
possibly leading to different nodes running different
versions of the image.

u30ysbxhf5kp35y7mpprexs7e
overall progress: 3 out of 3 tasks
1/3: running   [==================================================>]
2/3: running   [==================================================>]
3/3: running   [==================================================>]
verify: Service converged
[root@ip-10-0-1-114 ~]# docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                PORTS
u30ysbxhf5kp        weather-app         replicated          3/3                 weather-app:latest   *:80->3000/tcp

We can also paste one of the public IP addresses of our worker nodes in a browser to view the weather app.


## Add additional nodes to the swarm ##

[root@ip-10-0-1-142 ~]# echo "Swarm Server 2"
Swarm Server 2
[root@ip-10-0-1-142 ~]# docker swarm join --token SWMTKN-1-02ong7axnkouffgbxw43u6zowoa32w3iwdat7w7y44fud0k9bg-d8v6f2i0f6xsw7wj0lxgoe3gj 10.0.1.114:2377
This node joined a swarm as a worker.


[root@ip-10-0-1-164 ~]# echo "Swarm Server 3"
Swarm Server 3
[root@ip-10-0-1-164 ~]# docker swarm join --token SWMTKN-1-02ong7axnkouffgbxw43u6zowoa32w3iwdat7w7y44fud0k9bg-d8v6f2i0f6xsw7wj0lxgoe3gj 10.0.1.114:2377
This node joined a swarm as a worker


#########################################
Backing Up and Restoring a Docker Swarm
#########################################
For the last six months, the Acme Anvil Corporation has been migrating some of their bare metal infrastructure to Docker containers. A schism has developed between the members of your team on whether to use Docker Swarm or Kubernetes. To settle the dispute, your manager has decided to create a series of challenges. You have been tasked with creating a demo on how to back up and restore a Docker swarm. You are to set up a Docker swarm with 3 nodes, scale the backup service up to 3 nodes, and back up your master node and restore it to a backup instance.



## Join the worker nodes to the swarm ##

On server1, view the docker swarm join command:

[root@server1 ~]# cat swarm-token.txt
Copy the docker swarm join command provided in this output, and run it on server2:

[root@server2 ~]# docker swarm join --token $JOIN_TOKEN_HERE
Now we'll run it again on server3:

[root@server3 ~]# docker swarm join --token $JOIN_TOKEN_HERE
Back on the master node, let's view the running services:

[root@server1 ~]# docker service ls
We'll see a service named backup, and we want to scale it up to 3 nodes:

[root@server1 ~]# docker service scale backup=3
Now we can check by showing the service across our nodes:

[root@server1 ~]# docker service ps backup

## Back up the swarm master ## 

Before we can back up the Docker swarm, we've got to stop the docker service on the master node:

[root@server1 ~]# systemctl stop docker
Now back up the /var/lib/docker/swarm/ directory:

[root@server1 ~]# tar czvf swarm.tgz /var/lib/docker/swarm/

## Restore the swarm on the backup master ##

Let's copy the swarm backup from the master node to the backup master:

Note: Be sure to replace BACKUP_IP_ADDRESS with the private IP address of the backup server.

[root@server1 ~]# scp swarm.tgz cloud_user@BACKUP_IP_ADDRESS:/home/cloud_user/
Now we can get into the backup server and extract the backup file:

[root@backup ~]# cd /home/cloud_user
[root@backup ~]# tar xzvf swarm.tgz
Then copy the swarm directory to /var/lib/docker/swarm. First let's get into the directory:

[root@backup ~]# cd var/lib/docker
Now do the actual copying:

[root@backup ~]# cp -rf swarm/ /var/lib/docker/
With that finished, we can restart the Docker service:

[root@backup ~]# systemctl restart docker
Now we'll reinitialize the swarm:

[root@backup ~]# docker swarm init --force-new-cluster
We need the docker swarm join command that's in the output.

## Add the worker nodes to the restored cluster ##


On server2 and server3, we need to leave the first swarm and join the new one. First, server2:

[root@server2 ~]# docker swarm leave
[root@server2 ~]# docker swarm join --token $NEW_JOIN_TOKEN_HERE
Oops, that didn't work. Look at that new join token. The IP address is for server1. We need the IP for backup in there. Edit the command (swapping out the bad IP address for the good one) and try to join the swarm again.

Now let's repeat on server3:

[root@server3 ~]# docker swarm leave
[root@server3 ~]# docker swarm join --token $NEW_JOIN_TOKEN_HERE
Remember to change that IP again.

## Distribute the replicas across the swarm ##


On the backup server, make sure the backup service is still running:

[root@backup ~]# docker service ls
Now let's look at the current service list:

[root@backup ~]# docker service ps backup
This shows us that we have three replicas, but they're all on our backup master (backup). To make sure our service is distributed across our swarm, we've first got to scale things down to one, then back up to three:

[root@backup ~]# docker service scale backup=1
[root@backup ~]# docker service scale backup=3
Once that's done, let's check on our service again:

[root@backup ~]# docker service ps backup
We'll see that our service is now running across all of the nodes.

###############################
Scaling a Docker Swarm Service
###############################
We've got to create a couple of Docker swarm master nodes, and three worker nodes, then practice scaling replicas up and down.

There are five servers we'll need to log into: two will be swarm masters, and three will be swarm workers. Keep the Linux Academy lab page handy. We'll need it for login credentials, and to help keep track of which server is doing what. The command prompts in this lab guide are just general "user@host" types, but knowing which host has which IP will be helpful.

If we fire up five terminals, and log into each of the servers, we'll be able to switch back and forth quickly. And since we'll need to be root in all of them, we can just run a sudo su - as soon as we log into each one. Once we're in, we can get moving with Docker.

"HTTPD overview: Apache HTTP Server, also known as httpd (Hypertext Transfer Protocol Daemon), is a web server. It listens to one or more TCP ports (usually port 80) for incoming requests from browsers and serves the requested files accordingly. In addition to its basic operation, httpd can be extended with modules."

## Create a Swarm ##

On our first server, we're going to create a swarm master. Initialize the swarm with this:

[root@swarmmaster1]# docker swarm init
There's going to be some output, including a docker swarm join command that we can copy.

## Add Worker Nodes ##

On our worker node servers, we can join the swarm by pasting in the command that our swarm master spit out. Add the 3 worker nodes with these:

Worker 1
[root@swarmworker1]# docker swarm join --token <TOKEN> <IP_ADDRESS:2377>
Worker 2
[root@swarmworker2]# docker swarm join --token <TOKEN> <IP_ADDRESS:2377>
Worker 3
[root@swarmworker3]# docker swarm join --token <TOKEN> <IP_ADDRESS:2377>

## The Master Token ##

Back on swarmmaster1, we need to generate the master token:

[root@swarmmaster1]# docker swarm join-token manager
This will output yet another command. The first one we copied and pasted was for getting a worker node joined. This one is for when we want a master node to join. Let's log into the second master, and paste that command:

[root@swarmmaster2]# docker swarm join --token <TOKEN> <IP_ADDRESS:2377>

## Check Our Work ##

A quick docker node ls from our first master server will show us that there are five nodes running. Three are workers, and two have a MANAGER STATUS: Leader (the first master) and Reachable (the second master).

Ensure that the Two Masters Will Only Function as Masters
We want the masters to just be masters, not workers too. To do that we run these (one for each master):

Set the availability to drain for Master1.

[root@swarmmaster1]# docker node update --availability drain <MASTER1 ID>
Set the availability to drain for Master2.

[root@swarmmaster1]# docker node update --availability drain <MASTER2 ID>
<MASTER ID> here would be the ID column in that docker node ls command we ran earlier.

## Create a Service ##

We're going to create a service, and replicate it three times.

[root@swarmmaster1]# docker service create --name httpd -p 80:80 --replicas 3 httpd
In this command, we created the service, named it httpd, ran it on port 80, created three replicas, and used an image called httpd that was pulled from Docker Hub.

## Check What Is Running ##

We can look at what actually fired up with another docker command:

[root@swarmmaster1]# docker service ps httpd
The output shows that we have three replicas running, one on each of our worker nodes, and that they're only running on our worker nodes. Great. Now let's scale up.

## Scale the httpd Service up to Five Nodes ##

Instead of just running three replicas, let's scale the httpd service up to five:

[root@swarmmaster1]# docker service scale httpd=5
If we run another docker service ps httpd, we'll see there are now five replicas running. If we look at the nodes, we'll also see that it's still only the worker nodes running these. This is what we intended though: we want our master being masters, and just want the workers working.

Let's scale down and see what happens.

## Scale the httpd Service Back Down to Two Nodes ##

We're going to cut our workforce by three, so that we're only running two replicas:

[root@swarmmaster1]# docker service scale httpd=2
Now we can run docker service ps httpd again, and we'll see that three of the replicas have disappeared, leaving us with two.

